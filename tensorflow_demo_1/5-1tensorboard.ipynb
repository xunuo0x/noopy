{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "Iter 0,Testing Accuracy 0.9533 ,Traing Accuracy 0.95556366 ,Learning Rate 0.001\n",
      "Iter 1,Testing Accuracy 0.9638 ,Traing Accuracy 0.96943635 ,Learning Rate 0.00095\n",
      "Iter 2,Testing Accuracy 0.9685 ,Traing Accuracy 0.9761818 ,Learning Rate 0.0009025\n",
      "Iter 3,Testing Accuracy 0.9733 ,Traing Accuracy 0.98163635 ,Learning Rate 0.000857375\n",
      "Iter 4,Testing Accuracy 0.9738 ,Traing Accuracy 0.98587275 ,Learning Rate 0.00081450626\n",
      "Iter 5,Testing Accuracy 0.9748 ,Traing Accuracy 0.9892182 ,Learning Rate 0.0007737809\n",
      "Iter 6,Testing Accuracy 0.9781 ,Traing Accuracy 0.9904182 ,Learning Rate 0.0007350919\n",
      "Iter 7,Testing Accuracy 0.978 ,Traing Accuracy 0.9918182 ,Learning Rate 0.0006983373\n",
      "Iter 8,Testing Accuracy 0.9773 ,Traing Accuracy 0.9931091 ,Learning Rate 0.0006634204\n",
      "Iter 9,Testing Accuracy 0.9801 ,Traing Accuracy 0.9939273 ,Learning Rate 0.0006302494\n",
      "Iter 10,Testing Accuracy 0.9801 ,Traing Accuracy 0.9949818 ,Learning Rate 0.0005987369\n",
      "Iter 11,Testing Accuracy 0.9811 ,Traing Accuracy 0.9954364 ,Learning Rate 0.0005688001\n",
      "Iter 12,Testing Accuracy 0.9805 ,Traing Accuracy 0.9955818 ,Learning Rate 0.0005403601\n",
      "Iter 13,Testing Accuracy 0.9817 ,Traing Accuracy 0.99636364 ,Learning Rate 0.0005133421\n",
      "Iter 14,Testing Accuracy 0.9802 ,Traing Accuracy 0.99652725 ,Learning Rate 0.000487675\n",
      "Iter 15,Testing Accuracy 0.9817 ,Traing Accuracy 0.99683636 ,Learning Rate 0.00046329122\n",
      "Iter 16,Testing Accuracy 0.9832 ,Traing Accuracy 0.99683636 ,Learning Rate 0.00044012666\n",
      "Iter 17,Testing Accuracy 0.9813 ,Traing Accuracy 0.9968 ,Learning Rate 0.00041812033\n",
      "Iter 18,Testing Accuracy 0.9825 ,Traing Accuracy 0.9969818 ,Learning Rate 0.00039721432\n",
      "Iter 19,Testing Accuracy 0.9822 ,Traing Accuracy 0.9970545 ,Learning Rate 0.0003773536\n",
      "Iter 20,Testing Accuracy 0.9799 ,Traing Accuracy 0.9966182 ,Learning Rate 0.00035848594\n"
     ]
    }
   ],
   "source": [
    "# 载入数据集\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
    "\n",
    "# 每个批次的大小\n",
    "batch_size = 100\n",
    "\n",
    "# 计算一共多少个batch\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "# 命名空间\n",
    "with tf.name_scope('input'):\n",
    "    # 定义两个placeholder\n",
    "    x = tf.placeholder(tf.float32, [None, 784], 'x-input')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], 'y-input')\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "# 学习率\n",
    "lr = tf.Variable(0.001, dtype=tf.float32)\n",
    "\n",
    "# 创建一个简单的神经网络\n",
    "W1 = tf.Variable(tf.truncated_normal([784, 500], stddev = 0.1))\n",
    "b1 = tf.Variable(tf.zeros([500]) + 0.1)\n",
    "L1 = tf.nn.tanh(tf.matmul(x, W1) + b1)\n",
    "L1_drop = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([500, 300], stddev = 0.1))\n",
    "b2 = tf.Variable(tf.zeros([300]) + 0.1)\n",
    "L2 = tf.nn.tanh(tf.matmul(L1_drop, W2) + b2)\n",
    "L2_drop = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "W3 = tf.Variable(tf.truncated_normal([300, 10], stddev = 0.1))\n",
    "b3 = tf.Variable(tf.zeros([10]) + 0.1)\n",
    "prediction = tf.nn.sigmoid(tf.matmul(L2_drop, W3) + b3)\n",
    "\n",
    "# 二次代价函数\n",
    "#loss = tf.reduce_mean(tf.square(y-prediction))\n",
    "# 交叉熵代价函数\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))\n",
    "\n",
    "# 优化器：使用梯度下降法\n",
    "#train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "# 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 预测准确率\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction, 1))\n",
    "# 计算准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    writer = tf.summary.FileWriter('logs/',sess.graph)\n",
    "    for epoch in range(21):\n",
    "        sess.run(tf.assign(lr, 0.001*(0.95 ** epoch)))\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step, feed_dict={x: batch_xs, y:batch_ys, keep_prob: 1.0})\n",
    "        \n",
    "        learning_rate = sess.run(lr)\n",
    "        test_acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y:mnist.test.labels, keep_prob: 1.0})\n",
    "        train_acc = sess.run(accuracy, feed_dict={x: mnist.train.images, y:mnist.train.labels, keep_prob: 1.0})\n",
    "        print('Iter ' + str(epoch) + ',Testing Accuracy '+ str(test_acc) + ' ,Traing Accuracy '+ str(train_acc)+ ' ,Learning Rate '+ str(learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
